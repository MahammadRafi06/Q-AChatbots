{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_community.llms import Ollama\n",
    "api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "#llm = ChatGroq(groq_api_key=api_key, model_name=\"llama3-8b-8192\", streaming=True)\n",
    "llm = Ollama(model=\"llama3.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import ( \n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech = \"\"\"During his Farewell Hajj, Prophet Muhammad (PBUH) delivered a profound sermon that encapsulated the essential principles of Islam and laid \n",
    "the foundation for a just and equitable society. He emphasized the sanctity of human life, property, and honor, declaring them inviolable, much like the \n",
    "sacredness of the day of Arafah, the month of Dhul-Hijjah, and the city of Makkah. He abolished the practice of usury (riba), beginning with debts owed \n",
    "to his own family, thus setting a precedent for fairness in economic dealings. The Prophet (PBUH) called for the fair and respectful treatment of women,\n",
    "highlighting their rights and the responsibilities that men have towards them, reminding the community that women are a trust from Allah. He underscored\n",
    "the principle of equality among all people, stating that no Arab is superior to a non-Arab, nor is a non-Arab superior to an Arab, and that no person \n",
    "is superior to another based on race or color, but only through piety and righteous deeds. The Prophet (PBUH) urged the Muslim community to adhere to \n",
    "the Quran and his Sunnah, assuring them that by following these, they would never go astray. He emphasized the importance of unity and brotherhood \n",
    "among Muslims, warning against division and reminding them that they are one Ummah under Allah. He also addressed the treatment of slaves, calling \n",
    "for their kind and just treatment, as they are also part of the human family deserving of dignity. The Prophet (PBUH) abolished the pre-Islamic \n",
    "practice of revenge killings, nullifying all past blood feuds. He reminded the believers of their accountability to Allah for their actions and \n",
    "urged them to live in accordance with Islamic principles. In conclusion, he affirmed that he had conveyed the message of Islam in full and asked \n",
    "those present to bear witness to this, instructing them to pass on the message to those who were not present. This sermon, delivered on the 9th of \n",
    "Dhul-Hijjah, 10 AH, is a cornerstone of Islamic teachings, providing comprehensive guidance on morality, social justice, and human rights, and \n",
    "remains a key reference for Muslims around the world.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_mesasge=[SystemMessage(content=\"Yoo're an expert with expertize in summarizing the speeches\"), \n",
    "              HumanMessage(content=f\"Please provide short and concise summary of speech:{speech}\")\n",
    "              ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "477"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.get_num_tokens(speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Here is a concise summary of the speech:\\n\\nIn his Farewell Hajj sermon, Prophet Muhammad (PBUH) emphasized the sanctity of human life, property, and honor, and abolished usury and revenge killings. He stressed the importance of equality, unity, and brotherhood among Muslims, and the need to treat women and slaves with fairness and respect. He reminded the community to adhere to the Quran and his own teachings, and to live in accordance with Islamic principles. He also instructed the believers to bear witness to the message and pass it on to others. The sermon is considered a cornerstone of Islamic teachings, providing guidance on morality, social justice, and human rights.', response_metadata={'finish_reason': 'stop'}, id='run-12c1bfae-c5dd-40ee-802e-24a05eab8065-0', usage_metadata={'input_tokens': 501, 'output_tokens': 137, 'total_tokens': 638})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(chat_mesasge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['langauage', 'speech'], template=' write a summary of the following speeach:\\nSpeech:{speech}\\ntranslate the prcise summary to {langauage}\\n')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "generictemplate = \"\"\" write a summary of the following speeach:\n",
    "Speech:{speech}\n",
    "translate the prcise summary to {langauage}\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(input_variables=[\"speech\",\"langauage\"], template=generictemplate)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_prompt = prompt.format(speech=speech,langauage=\"French\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.get_num_tokens(complete_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here is a summary of the speech in Hindi:\\n\\nस्पीच: प्रोफ़ेट मोहम्मद (एसएबीयूएच) ने अपने अलविदा हज के दौरान एक संक्षिप्त सpeech दिया, जिसमें इस्लाम के मूल सिद्धांतों को समाहित किया गया था और एक न्यायपूर्ण और समान socialesociety की स्थापना की गई थी। उन्होंने इंसान के जीवन, संपत्ति और सम्मान की सantidad को Highlight किया, जिससे कि वे निविदा हों,一样arakah के दिन की तरह, धुल-हिज्जा के महीने की तरह और मक्का की शहर की तरह। उन्होंने ऋण (रिबा) की पрак्स को समाप्त कर दिया, जहां उनके परिवार से होने वाले ऋण शुरू किए गए, जिससे कि आर्थिक सौदों में न्यायपूर्णता का संकेत मिला। प्रोफ़ेट ने महिलाओं के सम्मान और उनके अधिकारों की मांग की, जिसमें उन्होंने पुरुषों के प्रति उनके जिम्मेदारियों को भी Highlight किया, और महिलाओं को अल्लाह की लाज के रूप में संबोधित किया। उन्होंने सभी लोगों में समानता की बात कही, और कहा कि कोई अरब नहीं है जो एक नारब नहीं है, और कोई नारब नहीं है जो एक अरब नहीं है, और कोई व्यक्ति नहीं है जो दूसरे व्यक्ति से अधिक है, सिवाय अल्लाह की पसंदगी और नेक काम से। प्रोफ़ेट ने मुस्लिम समुदाय को कुरान और उनके सुन्नत का पालन करने को कहा, और उन्हें आश्वासन दिया कि यदि वे इनका पालन करते हैं, तो वे कभी भटक नहीं जाएंगे। उन्होंने मुस्लिमों के बीच एकता और भाईचारे की हिदायत की, और उन्हें चेतावनी दी कि वे टुकड़े-टुकड़े हो जाएंगे और अल्लाह के नाम पर एक हैं। उन्होंने गुलामों के साथ न्यायपूर्ण और किए गए संबंध की मांग की, और कहा कि वे भी इंसानी परिवार का हिस्सा हैं, और उनको सम्मान की जरूरत है। प्रोफ़ेट ने पुराने खूनी मामलों को समाप्त कर दिया, और लोगों को अल्लाह के लिए जवाबदेही की याद दिलायी। अंत में, उन्होंने कहा कि वे इस्लाम की संदेश को पूरा किया है और मौजूद लोगों सेewitness के लिए कहा, और उन्हें निर्देश दिया कि वे जो लोग मौजूद नहीं थे, उन्हें संदेश पहुंचाएं। यह स्पीच, जो 10 एच के 9वें दिन धुल-हिज्जा को दिया गया था, इस्लाम के सिद्धांतों का एक स्तम्भ है, जिसमें मораль, सामाजिक न्याय और इंसान अधिकारों की hướng दी गई है, और आज भी मुस्लिमों के लिए एक मुख्य संदर्भ है।'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain =LLMChain(llm=llm, prompt=prompt)\n",
    "summary = llm_chain.run({\"speech\":speech, \"langauage\":\"hindi\"})\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"LLM.pdf\")\n",
    "docs=loader.load_and_split()\n",
    "docs = docs[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\" Write a concise short summary of the foloowing documents:{text} \n",
    "\"\"\"\n",
    "prompt = PromptTemplate(input_variable=[\"text\"], template=template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mrafi\\Documents\\LangchainProjects\\1-Q&AChatbots\\AIProjects\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:141: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m Write a concise short summary of the foloowing documents:A Comprehensive Overview of Large Language Models\n",
      "Humza Naveeda, Asad Ullah Khana,∗, Shi Qiub,∗, Muhammad Saqibc,d,∗, Saeed Anware,f, Muhammad Usmane,f, Naveed Akhtarg,i,\n",
      "Nick Barnesh, Ajmal Miani\n",
      "aUniversity of Engineering and Technology (UET), Lahore, Pakistan\n",
      "bThe Chinese University of Hong Kong (CUHK), HKSAR, China\n",
      "cUniversity of Technology Sydney (UTS), Sydney, Australia\n",
      "dCommonwealth Scientific and Industrial Research Organisation (CSIRO), Sydney, Australia\n",
      "eKing Fahd University of Petroleum and Minerals (KFUPM), Dhahran, Saudi Arabia\n",
      "fSDAIA-KFUPM Joint Research Center for Artificial Intelligence (JRCAI), Dhahran, Saudi Arabia\n",
      "gThe University of Melbourne (UoM), Melbourne, Australia\n",
      "hAustralian National University (ANU), Canberra, Australia\n",
      "iThe University of Western Australia (UWA), Perth, Australia\n",
      "Abstract\n",
      "Large Language Models (LLMs) have recently demonstrated remarkable capabilities in natural language processing tasks and\n",
      "beyond. This success of LLMs has led to a large influx of research contributions in this direction. These works encompass diverse\n",
      "topics such as architectural innovations, better training strategies, context length improvements, fine-tuning, multi-modal LLMs,\n",
      "robotics, datasets, benchmarking, e fficiency, and more. With the rapid development of techniques and regular breakthroughs in\n",
      "LLM research, it has become considerably challenging to perceive the bigger picture of the advances in this direction. Considering\n",
      "the rapidly emerging plethora of literature on LLMs, it is imperative that the research community is able to benefit from a concise\n",
      "yet comprehensive overview of the recent developments in this field. This article provides an overview of the existing literature\n",
      "on a broad range of LLM-related concepts. Our self-contained comprehensive overview of LLMs discusses relevant background\n",
      "concepts along with covering the advanced topics at the frontier of research in LLMs. This review article is intended to not only\n",
      "provide a systematic survey but also a quick comprehensive reference for the researchers and practitioners to draw insights from\n",
      "extensive informative summaries of the existing works to advance the LLM research.\n",
      "Keywords:\n",
      "Large Language Models, LLMs, chatGPT, Augmented LLMs, Multimodal LLMs, LLM training, LLM Benchmarking\n",
      "1. Introduction\n",
      "Language plays a fundamental role in facilitating commu-\n",
      "nication and self-expression for humans, and their interaction\n",
      "with machines. The need for generalized models stems from\n",
      "the growing demand for machines to handle complex language\n",
      "tasks, including translation, summarization, information re-\n",
      "trieval, conversational interactions, etc. Recently, significant\n",
      "breakthroughs have been witnessed in language models, pri-\n",
      "marily attributed to transformers [1], increased computational\n",
      "capabilities, and the availability of large-scale training data.\n",
      "These developments have brought about a revolutionary trans-\n",
      "formation by enabling the creation of LLMs that can approxi-\n",
      "mate human-level performance on various tasks [2, 3]. Large\n",
      "∗Equal contribution\n",
      "Email addresses: humza_naveed@yahoo.com (Humza Naveed),\n",
      "aukhanee@gmail.com (Asad Ullah Khan), shiqiu@cse.cuhk.edu.hk (Shi\n",
      "Qiu), muhammad.saqib@data61.csiro.au (Muhammad Saqib),\n",
      "saeed.anwar@kfupm.edu.sa (Saeed Anwar),\n",
      "muhammad.usman@kfupm.edu.sa (Muhammad Usman),\n",
      "naveed.akhtar1@unimelb.edu.au (Naveed Akhtar),\n",
      "nick.barnes@anu.edu.au (Nick Barnes), ajmal.mian@uwa.edu.au\n",
      "(Ajmal Mian)\n",
      "Figure 1: The trend of papers released over years containing keywords \"Large\n",
      "Language Model\", \"Large Language Model +Fine-Tuning\", and \"Large Lan-\n",
      "guage Model +Alignment\".\n",
      "Preprint submitted to Elsevier April 11, 2024arXiv:2307.06435v9  [cs.CL]  9 Apr 2024\n",
      "\n",
      "2019T5(Oct)\n",
      "GPT-3(May)\n",
      " WebGPT (Dec)\n",
      "OPT -IML\n",
      "TK-Instruct (May)\n",
      "mT0 (Dec)\n",
      " Wizard -LM\n",
      "Vicuna\n",
      "Alpaca (Mar)\n",
      "HuaTuo (Apr)\n",
      "Koala (May)\n",
      "Wizard -Coder (Jun)\n",
      "Goat\n",
      "PanGu -α(Apr)\n",
      "CPM -2(Jun)\n",
      "GPT-NeoX -20B (Apr)\n",
      "CodeGen (Mar)\n",
      "Galactica (Nov)\n",
      "GLM (Oct)\n",
      "OPT\n",
      "UL2 (May)\n",
      "LLaMA (Feb)\n",
      "LLaMA 2(Jul)\n",
      "MPT (Jun)\n",
      "CodeT5+\n",
      "Code Llama (Aug)\n",
      "StarCoder\n",
      "Xuan Yuan 2.0 (May)\n",
      "2020 2021 2022 2023 2024mT5 (Oct)\n",
      "HyperCLOVA (Sep)\n",
      "ERNIE 3.0\n",
      "Codex (Jul)\n",
      "Jurassic -1(Aug)\n",
      "Yuan 1.0 (Oct)\n",
      "Gopher (Dec)\n",
      "ERNIE 3.0 Titan\n",
      "GLaM\n",
      "LaMDA\n",
      "T0(Oct)\n",
      "ChatGPT (Nov)\n",
      "Sparrow (Sep)\n",
      "FLAN -U-PaLM (Oct)\n",
      "Bard (Oct)\n",
      "MT-NLG (Jan)\n",
      "AlphaCode (Feb)\n",
      "Chinchilla (Mar)\n",
      "PaLM (Apr)\n",
      "U-PALM (Oct)\n",
      "BLOOM (Nov)\n",
      "AlexaTM (Aug)\n",
      " PaLM2 (May)\n",
      "GPT-4\n",
      "PanGu -Σ(Mar)\n",
      "BloombergGPT\n",
      "Claude\n",
      "Gemini (Dec)\n",
      "Figure 2: Chronological display of LLM releases: blue cards represent ‘pre-trained’ models, while orange cards correspond to ‘instruction-tuned’ models. Models\n",
      "on the upper half signify open-source availability, whereas those on the bottom half are closed-source. The chart illustrates the increasing trend towards instruction-\n",
      "tuned models and open-source models, highlighting the evolving landscape and trends in natural language processing research.\n",
      "Language Models (LLMs) have emerged as cutting-edge arti-\n",
      "ficial intelligence systems that can process and generate text\n",
      "with coherent communication [4], and generalize to multiple\n",
      "tasks [5, 6].\n",
      "The historical progress in natural language processing (NLP)\n",
      "evolved from statistical to neural language modeling and then\n",
      "from pre-trained language models (PLMs) to LLMs. While\n",
      "conventional language modeling (LM) trains task-specific mod-\n",
      "els in supervised settings, PLMs are trained in a self-supervised\n",
      "setting on a large corpus of text [7, 8, 9] with the aim of learning\n",
      "a generic representation that is shareable among various NLP\n",
      "tasks. After fine-tuning for downstream tasks, PLMs surpass\n",
      "the performance gains of traditional language modeling (LM).\n",
      "The larger PLMs bring more performance gains, which has led\n",
      "to the transitioning of PLMs to LLMs by significantly increas-\n",
      "ing model parameters (tens to hundreds of billions) [10] and\n",
      "training dataset (many GBs and TBs) [10, 11]. Following this\n",
      "development, numerous LLMs have been proposed in the lit-\n",
      "erature [10, 11, 12, 6, 13, 14, 15]. An increasing trend in the\n",
      "number of released LLMs and names of a few significant LLMs\n",
      "proposed over the years are shown in Fig 1 and Fig 2, respec-\n",
      "tively.\n",
      "The early work on LLMs, such as T5 [10] and mT5 [11] em-\n",
      "ployed transfer learning until GPT-3 [6] showed LLMs are\n",
      "zero-shot transferable to downstream tasks without fine-tuning.\n",
      "LLMs accurately respond to task queries when prompted with\n",
      "task descriptions and examples. However, pre-trained LLMs\n",
      "fail to follow user intent and perform worse in zero-shot set-\n",
      "tings than in few-shot. Fine-tuning them with task instruc-\n",
      "tions data [16, 17, 18, 19] and aligning with human prefer-\n",
      "ences [20, 21] enhances generalization to unseen tasks, im-\n",
      "proving zero-shot performance significantly and reducing mis-\n",
      "aligned behavior.\n",
      "In addition to better generalization and domain adaptation,\n",
      "LLMs appear to have emergent abilities, such as reasoning,\n",
      "planning, decision-making, in-context learning, answering in\n",
      "zero-shot settings, etc. These abilities are known to be ac-\n",
      "quired by them due to their gigantic scale even when the pre-\n",
      "trained LLMs are not trained specifically to possess these at-\n",
      "tributes [22, 23, 24]. Such abilities have led LLMs to be widelyadopted in diverse settings including, multi-modal, robotics,\n",
      "tool manipulation, question answering, autonomous agents, etc.\n",
      "Various improvements have also been suggested in these areas\n",
      "either by task-specific training [25, 26, 27, 28, 29, 30, 31] or\n",
      "better prompting [32].\n",
      "The LLMs abilities to solve diverse tasks with human-level\n",
      "performance come at a cost of slow training and inference,\n",
      "extensive hardware requirements, and higher running costs.\n",
      "Such requirements have limited their adoption and opened up\n",
      "\n",
      "performance come at a cost of slow training and inference,\n",
      "extensive hardware requirements, and higher running costs.\n",
      "Such requirements have limited their adoption and opened up\n",
      "opportunities to devise better architectures [15, 33, 34, 35]\n",
      "and training strategies [36, 37, 21, 38, 39, 40, 41]. Param-\n",
      "eter e fficient tuning [38, 41, 40], pruning [42, 43], quantiza-\n",
      "tion [44, 45], knowledge distillation, and context length inter-\n",
      "polation [46, 47, 48, 49] among others are some of the methods\n",
      "widely studied for e fficient LLM utilization.\n",
      "Due to the success of LLMs on a wide variety of tasks, the\n",
      "research literature has recently experienced a large influx of\n",
      "LLM-related contributions. Researchers have organized the\n",
      "LLMs literature in surveys [50, 51, 52, 53], and topic-specific\n",
      "surveys in [54, 55, 56, 57, 58]. In contrast to these surveys, our\n",
      "contribution focuses on providing a comprehensive yet concise\n",
      "overview of the general direction of LLM research. This arti-\n",
      "cle summarizes architectural and training details of pre-trained\n",
      "LLMs and delves deeper into the details of concepts like fine-\n",
      "tuning, multi-modal LLMs, augmented LLMs, datasets, eval-\n",
      "uation, applications, challenges, and others to provide a self-\n",
      "contained comprehensive overview. Our key contributions are\n",
      "summarized as follows.\n",
      "•We present a survey on the developments in LLM research\n",
      "providing a concise comprehensive overview of the direc-\n",
      "tion.\n",
      "•We present extensive summaries of pre-trained models that\n",
      "include fine-grained details of architecture and training de-\n",
      "tails.\n",
      "•We summarize major findings of the popular contributions\n",
      "and provide a detailed discussion on the key design and\n",
      "development aspects of LLMs to help practitioners e ffec-\n",
      "tively leverage this technology.\n",
      "•In this self-contained article, we cover a range of con-\n",
      "cepts to present the general direction of LLMs compre-\n",
      "2\n",
      "\n",
      "Figure 3: A broader overview of LLMs, dividing LLMs into seven branches: 1. Pre-Training 2. Fine-Tuning 3. E fficient 4. Inference 5. Evaluation 6. Applications\n",
      "7. Challenges\n",
      "hensively, including background, pre-training, fine-tuning,\n",
      "multi-modal LLMs, augmented LLMs, LLMs-powered\n",
      "agents, datasets, evaluation, etc.\n",
      "We loosely follow the existing terminology to ensure a stan-\n",
      "dardized outlook of this research direction. For instance, fol-\n",
      "lowing [50], our survey discusses pre-trained LLMs with 10B\n",
      "parameters or more. We refer the readers interested in smaller\n",
      "pre-trained models to [51, 52, 53].\n",
      "The organization of this paper is as follows. Section 2 discusses\n",
      "the background of LLMs. Section 3 focuses on LLMs overview,architectures, training pipelines and strategies, fine-tuning, and\n",
      "utilization in di fferent domains. Section 4 highlights the config-\n",
      "uration and parameters that play a crucial role in the function-\n",
      "ing of these models. Summary and discussions are presented\n",
      "in section 3.8. The LLM training and evaluation, datasets, and\n",
      "benchmarks are discussed in section 5, followed by challenges\n",
      "and future directions, and conclusion in sections 7 and 8, re-\n",
      "spectively.\n",
      "3\n",
      "\n",
      "2. Background\n",
      "We provide the relevant background to understand the fun-\n",
      "damentals related to LLMs in this section. We briefly discuss\n",
      "necessary components in LLMs and refer the readers interested\n",
      "in details to the original works.\n",
      "2.1. Tokenization\n",
      "Tokenization [59] is an essential pre-processing step in\n",
      "LLM training that parses the text into non-decomposing units\n",
      "called tokens. Tokens can be characters, subwords [60], sym-\n",
      "bols [61], or words, depending on the tokenization process.\n",
      "Some of the commonly used tokenization schemes in LLMs\n",
      "include wordpiece [62], byte pair encoding (BPE) [61], and un-\n",
      "igramLM [60]. Readers are encouraged to refer to [63] for a\n",
      "detailed survey.\n",
      "2.2. Encoding Positions\n",
      "The transformer processes input sequences in parallel and\n",
      "independently of each other. Moreover, the attention mod-\n",
      "ule in the transformer does not capture positional information.\n",
      "As a result, positional encodings were introduced in trans-\n",
      "former [64], where a positional embedding vector is added to\n",
      "the token embedding. Variants of positional embedding include\n",
      "absolute, relative, or learned positional encodings. Within rel-\n",
      "ative encoding, Alibi and RoPE are two widely used positional\n",
      "embeddings in LLMs.\n",
      "Alibi [65]: It subtracts a scalar bias from the attention score\n",
      "that increases with the distance between token positions. This\n",
      "favors using recent tokens for attention.\n",
      "RoPE [66]: It rotates query and key representations at an an-\n",
      "gle proportional to the token absolute position in the input\n",
      "sequence, resulting in a relative positional encoding scheme\n",
      "which decays with the distance between the tokens.\n",
      "2.3. Attention in LLMs\n",
      "Attention assigns weights to input tokens based on impor-\n",
      "tance so that the model gives more emphasis to relevant tokens.\n",
      "Attention in transformers [64] calculates query, key, and value\n",
      "mappings for input sequences, where the attention score is\n",
      "obtained by multiplying the query and key, and later used to\n",
      "weight values. We discuss di fferent attention strategies used in\n",
      "LLMs below.\n",
      "Self-Attention [64]: Calculates attention using queries, keys,\n",
      "and values from the same block (encoder or decoder).\n",
      "Cross Attention: It is used in encoder-decoder architectures,\n",
      "where encoder outputs are the queries, and key-value pairs\n",
      "come from the decoder.\n",
      "Sparse Attention [67]: Self-attention has O(n2) time complex-\n",
      "ity which becomes infeasible for large sequences. To speed\n",
      "up the computation, sparse attention [67] iteratively calculates\n",
      "attention in sliding windows for speed gains.\n",
      "Flash Attention [68]: Memory access is the major bottleneck\n",
      "in calculating attention using GPUs. To speed up, flash\n",
      "attention employs input tiling to minimize the memory reads\n",
      "and writes between the GPU high bandwidth memory (HBM)\n",
      "and the on-chip SRAM.2.4. Activation Functions\n",
      "The activation functions serve a crucial role in the curve-\n",
      "fitting abilities of neural networks [69]. We discuss activation\n",
      "functions used in LLMs in this section.\n",
      "ReLU [70]: The Rectified linear unit (ReLU) is defined as:\n",
      "ReLU (x)=max(0,x) (1)\n",
      "GeLU [71]: The Gaussian Error Linear Unit (GeLU) is the\n",
      "combination of ReLU, dropout [72] and zoneout [73].\n",
      "GLU variants [74]: The Gated Linear Unit [75] is a neural\n",
      "network layer that is an element-wise product ( ⊗) of a linear\n",
      "transformation and a sigmoid transformed ( σ) linear projection\n",
      "of the input given as:\n",
      "GLU (x,W,V,b,c)=(xW+b)⊗σ(xV+c), (2)\n",
      "where Xis the input of layer and l,W,b,Vandcare learned\n",
      "parameters. Other GLU variants [74] used in LLMs are:\n",
      "ReGLU (x,W,V,b,c)=max(0,xW+b)⊗,\n",
      "GEGLU (x,W,V,b,c)=GELU (xW+b)⊗(xV+c),\n",
      "S wiGLU (x,W,V,b,c,β)=S wishβ(xW+b)⊗(xV+c).\n",
      "2.5. Layer Normalization\n",
      "Layer normalization leads to faster convergence and is an in-\n",
      "tegrated component of transformers [64]. In addition to Layer-\n",
      "Norm [76] and RMSNorm [77], LLMs use pre-layer normal-\n",
      "ization [78], applying it before multi-head attention (MHA).\n",
      "Pre-norm is shown to provide training stability in LLMs. An- \n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"This text appears to be a snippet from an academic article or research paper related to Large Language Models (LLMs). I'll break down the content for you:\\n\\n**Section 2: Background**\\n\\nThe section provides an overview of the necessary components and concepts related to LLMs.\\n\\n**Subsection 2.1: Tokenization**\\n\\nTokenization is a crucial step in LLM training that breaks down text into non-decomposing units called tokens. The authors mention various tokenization schemes used in LLMs, including wordpiece, byte pair encoding (BPE), and unigram LM.\\n\\n**Subsection 2.2: Encoding Positions**\\n\\nThe transformer architecture processes input sequences independently and does not capture positional information. To address this, the authors discuss positional encodings, which are added to token embeddings. They mention three types of positional embeddings used in LLMs:\\n\\n1. Alibi (subtracts a scalar bias from attention scores)\\n2. RoPE (rotates query and key representations at an angle proportional to token absolute position)\\n\\n**Subsection 2.3: Attention in LLMs**\\n\\nAttention is a mechanism that assigns weights to input tokens based on importance. The authors discuss different types of attention used in LLMs:\\n\\n1. Self-attention (calculates attention using queries, keys, and values from the same block)\\n2. Cross attention (used in encoder-decoder architectures)\\n3. Sparse attention (speeds up computation by iterating over sliding windows)\\n4. Flash attention (minimizes memory reads and writes between GPU HBM and on-chip SRAM)\\n\\n**Subsection 2.4: Activation Functions**\\n\\nThe authors discuss various activation functions used in LLMs, including:\\n\\n1. ReLU (Rectified linear unit)\\n2. GeLU (Gaussian Error Linear Unit)\\n3. GLU variants (Gated Linear Unit) - three types are mentioned:\\n\\t* ReGLU\\n\\t* GEGLU\\n\\t* S wiGLU\\n\\n**Subsection 2.5: Layer Normalization**\\n\\nLayer normalization is a technique that leads to faster convergence and is integrated in transformers. The authors mention two types of layer normalization used in LLMs:\\n\\n1. Layer-Norm (LN)\\n2. RMSNorm\\n3. Pre-layer normalization (applying normalization before multi-head attention)\\n\\nThe text provides an overview of the fundamental components and concepts related to Large Language Models, which will likely be useful for readers interested in understanding the basics of this area of research.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "chain  = load_summarize_chain(llm,chain_type=\"stuff\",prompt=prompt, verbose=True)\n",
    "output_summary = chain.run(docs)\n",
    "output_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"LLM.pdf\")\n",
    "docs=loader.load_and_split()\n",
    "docs = docs[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "final_docs = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100).split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_template = \"\"\"\" Please summarize the below docs: \n",
    "docs:{text}\n",
    "Summary: \n",
    "\"\"\"\n",
    "map_prompt_template = PromptTemplate(input_variable=['text'], template=chunk_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_promt = \"\"\"\" Provide the final summary of the entire documents with imparttant points and entities which were specifically mentioned in the documnets\n",
    "docs:{text} \n",
    "\"\"\"\n",
    "final_prompt_template = PromptTemplate(input_variable=['text'],template=final_promt )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\" Please summarize the below docs: \n",
      "docs:A Comprehensive Overview of Large Language Models\n",
      "Humza Naveeda, Asad Ullah Khana,∗, Shi Qiub,∗, Muhammad Saqibc,d,∗, Saeed Anware,f, Muhammad Usmane,f, Naveed Akhtarg,i,\n",
      "Nick Barnesh, Ajmal Miani\n",
      "aUniversity of Engineering and Technology (UET), Lahore, Pakistan\n",
      "bThe Chinese University of Hong Kong (CUHK), HKSAR, China\n",
      "cUniversity of Technology Sydney (UTS), Sydney, Australia\n",
      "dCommonwealth Scientific and Industrial Research Organisation (CSIRO), Sydney, Australia\n",
      "Summary: \n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\" Please summarize the below docs: \n",
      "docs:dCommonwealth Scientific and Industrial Research Organisation (CSIRO), Sydney, Australia\n",
      "eKing Fahd University of Petroleum and Minerals (KFUPM), Dhahran, Saudi Arabia\n",
      "fSDAIA-KFUPM Joint Research Center for Artificial Intelligence (JRCAI), Dhahran, Saudi Arabia\n",
      "gThe University of Melbourne (UoM), Melbourne, Australia\n",
      "hAustralian National University (ANU), Canberra, Australia\n",
      "iThe University of Western Australia (UWA), Perth, Australia\n",
      "Abstract\n",
      "Summary: \n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\" Please summarize the below docs: \n",
      "docs:iThe University of Western Australia (UWA), Perth, Australia\n",
      "Abstract\n",
      "Large Language Models (LLMs) have recently demonstrated remarkable capabilities in natural language processing tasks and\n",
      "beyond. This success of LLMs has led to a large influx of research contributions in this direction. These works encompass diverse\n",
      "topics such as architectural innovations, better training strategies, context length improvements, fine-tuning, multi-modal LLMs,\n",
      "Summary: \n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\" Please summarize the below docs: \n",
      "docs:robotics, datasets, benchmarking, e fficiency, and more. With the rapid development of techniques and regular breakthroughs in\n",
      "LLM research, it has become considerably challenging to perceive the bigger picture of the advances in this direction. Considering\n",
      "the rapidly emerging plethora of literature on LLMs, it is imperative that the research community is able to benefit from a concise\n",
      "Summary: \n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\" Please summarize the below docs: \n",
      "docs:yet comprehensive overview of the recent developments in this field. This article provides an overview of the existing literature\n",
      "on a broad range of LLM-related concepts. Our self-contained comprehensive overview of LLMs discusses relevant background\n",
      "concepts along with covering the advanced topics at the frontier of research in LLMs. This review article is intended to not only\n",
      "Summary: \n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\" Please summarize the below docs: \n",
      "docs:provide a systematic survey but also a quick comprehensive reference for the researchers and practitioners to draw insights from\n",
      "extensive informative summaries of the existing works to advance the LLM research.\n",
      "Keywords:\n",
      "Large Language Models, LLMs, chatGPT, Augmented LLMs, Multimodal LLMs, LLM training, LLM Benchmarking\n",
      "1. Introduction\n",
      "Language plays a fundamental role in facilitating commu-\n",
      "nication and self-expression for humans, and their interaction\n",
      "Summary: \n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\" Please summarize the below docs: \n",
      "docs:nication and self-expression for humans, and their interaction\n",
      "with machines. The need for generalized models stems from\n",
      "the growing demand for machines to handle complex language\n",
      "tasks, including translation, summarization, information re-\n",
      "trieval, conversational interactions, etc. Recently, significant\n",
      "breakthroughs have been witnessed in language models, pri-\n",
      "marily attributed to transformers [1], increased computational\n",
      "capabilities, and the availability of large-scale training data.\n",
      "Summary: \n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\" Please summarize the below docs: \n",
      "docs:capabilities, and the availability of large-scale training data.\n",
      "These developments have brought about a revolutionary trans-\n",
      "formation by enabling the creation of LLMs that can approxi-\n",
      "mate human-level performance on various tasks [2, 3]. Large\n",
      "∗Equal contribution\n",
      "Email addresses: humza_naveed@yahoo.com (Humza Naveed),\n",
      "aukhanee@gmail.com (Asad Ullah Khan), shiqiu@cse.cuhk.edu.hk (Shi\n",
      "Qiu), muhammad.saqib@data61.csiro.au (Muhammad Saqib),\n",
      "saeed.anwar@kfupm.edu.sa (Saeed Anwar),\n",
      "Summary: \n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\" Please summarize the below docs: \n",
      "docs:Qiu), muhammad.saqib@data61.csiro.au (Muhammad Saqib),\n",
      "saeed.anwar@kfupm.edu.sa (Saeed Anwar),\n",
      "muhammad.usman@kfupm.edu.sa (Muhammad Usman),\n",
      "naveed.akhtar1@unimelb.edu.au (Naveed Akhtar),\n",
      "nick.barnes@anu.edu.au (Nick Barnes), ajmal.mian@uwa.edu.au\n",
      "(Ajmal Mian)\n",
      "Figure 1: The trend of papers released over years containing keywords \"Large\n",
      "Language Model\", \"Large Language Model +Fine-Tuning\", and \"Large Lan-\n",
      "guage Model +Alignment\".\n",
      "Summary: \n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\" Please summarize the below docs: \n",
      "docs:Language Model\", \"Large Language Model +Fine-Tuning\", and \"Large Lan-\n",
      "guage Model +Alignment\".\n",
      "Preprint submitted to Elsevier April 11, 2024arXiv:2307.06435v9  [cs.CL]  9 Apr 2024\n",
      "Summary: \n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\" Please summarize the below docs: \n",
      "docs:2019T5(Oct)\n",
      "GPT-3(May)\n",
      " WebGPT (Dec)\n",
      "OPT -IML\n",
      "TK-Instruct (May)\n",
      "mT0 (Dec)\n",
      " Wizard -LM\n",
      "Vicuna\n",
      "Alpaca (Mar)\n",
      "HuaTuo (Apr)\n",
      "Koala (May)\n",
      "Wizard -Coder (Jun)\n",
      "Goat\n",
      "PanGu -α(Apr)\n",
      "CPM -2(Jun)\n",
      "GPT-NeoX -20B (Apr)\n",
      "CodeGen (Mar)\n",
      "Galactica (Nov)\n",
      "GLM (Oct)\n",
      "OPT\n",
      "UL2 (May)\n",
      "LLaMA (Feb)\n",
      "LLaMA 2(Jul)\n",
      "MPT (Jun)\n",
      "CodeT5+\n",
      "Code Llama (Aug)\n",
      "StarCoder\n",
      "Xuan Yuan 2.0 (May)\n",
      "2020 2021 2022 2023 2024mT5 (Oct)\n",
      "HyperCLOVA (Sep)\n",
      "ERNIE 3.0\n",
      "Codex (Jul)\n",
      "Jurassic -1(Aug)\n",
      "Yuan 1.0 (Oct)\n",
      "Gopher (Dec)\n",
      "ERNIE 3.0 Titan\n",
      "GLaM\n",
      "LaMDA\n",
      "T0(Oct)\n",
      "Summary: \n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\" Please summarize the below docs: \n",
      "docs:Codex (Jul)\n",
      "Jurassic -1(Aug)\n",
      "Yuan 1.0 (Oct)\n",
      "Gopher (Dec)\n",
      "ERNIE 3.0 Titan\n",
      "GLaM\n",
      "LaMDA\n",
      "T0(Oct)\n",
      "ChatGPT (Nov)\n",
      "Sparrow (Sep)\n",
      "FLAN -U-PaLM (Oct)\n",
      "Bard (Oct)\n",
      "MT-NLG (Jan)\n",
      "AlphaCode (Feb)\n",
      "Chinchilla (Mar)\n",
      "PaLM (Apr)\n",
      "U-PALM (Oct)\n",
      "BLOOM (Nov)\n",
      "AlexaTM (Aug)\n",
      " PaLM2 (May)\n",
      "GPT-4\n",
      "PanGu -Σ(Mar)\n",
      "BloombergGPT\n",
      "Claude\n",
      "Gemini (Dec)\n",
      "Figure 2: Chronological display of LLM releases: blue cards represent ‘pre-trained’ models, while orange cards correspond to ‘instruction-tuned’ models. Models\n",
      "Summary: \n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\" Please summarize the below docs: \n",
      "docs:on the upper half signify open-source availability, whereas those on the bottom half are closed-source. The chart illustrates the increasing trend towards instruction-\n",
      "tuned models and open-source models, highlighting the evolving landscape and trends in natural language processing research.\n",
      "Language Models (LLMs) have emerged as cutting-edge arti-\n",
      "ficial intelligence systems that can process and generate text\n",
      "with coherent communication [4], and generalize to multiple\n",
      "tasks [5, 6].\n",
      "Summary: \n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\" Please summarize the below docs: \n",
      "docs:with coherent communication [4], and generalize to multiple\n",
      "tasks [5, 6].\n",
      "The historical progress in natural language processing (NLP)\n",
      "evolved from statistical to neural language modeling and then\n",
      "from pre-trained language models (PLMs) to LLMs. While\n",
      "conventional language modeling (LM) trains task-specific mod-\n",
      "els in supervised settings, PLMs are trained in a self-supervised\n",
      "setting on a large corpus of text [7, 8, 9] with the aim of learning\n",
      "Summary: \n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\" Please summarize the below docs: \n",
      "docs:setting on a large corpus of text [7, 8, 9] with the aim of learning\n",
      "a generic representation that is shareable among various NLP\n",
      "tasks. After fine-tuning for downstream tasks, PLMs surpass\n",
      "the performance gains of traditional language modeling (LM).\n",
      "The larger PLMs bring more performance gains, which has led\n",
      "to the transitioning of PLMs to LLMs by significantly increas-\n",
      "ing model parameters (tens to hundreds of billions) [10] and\n",
      "training dataset (many GBs and TBs) [10, 11]. Following this\n",
      "Summary: \n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\" Please summarize the below docs: \n",
      "docs:training dataset (many GBs and TBs) [10, 11]. Following this\n",
      "development, numerous LLMs have been proposed in the lit-\n",
      "erature [10, 11, 12, 6, 13, 14, 15]. An increasing trend in the\n",
      "number of released LLMs and names of a few significant LLMs\n",
      "proposed over the years are shown in Fig 1 and Fig 2, respec-\n",
      "tively.\n",
      "The early work on LLMs, such as T5 [10] and mT5 [11] em-\n",
      "ployed transfer learning until GPT-3 [6] showed LLMs are\n",
      "zero-shot transferable to downstream tasks without fine-tuning.\n",
      "Summary: \n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\" Please summarize the below docs: \n",
      "docs:zero-shot transferable to downstream tasks without fine-tuning.\n",
      "LLMs accurately respond to task queries when prompted with\n",
      "task descriptions and examples. However, pre-trained LLMs\n",
      "fail to follow user intent and perform worse in zero-shot set-\n",
      "tings than in few-shot. Fine-tuning them with task instruc-\n",
      "tions data [16, 17, 18, 19] and aligning with human prefer-\n",
      "ences [20, 21] enhances generalization to unseen tasks, im-\n",
      "proving zero-shot performance significantly and reducing mis-\n",
      "Summary: \n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\" Please summarize the below docs: \n",
      "docs:proving zero-shot performance significantly and reducing mis-\n",
      "aligned behavior.\n",
      "In addition to better generalization and domain adaptation,\n",
      "LLMs appear to have emergent abilities, such as reasoning,\n",
      "planning, decision-making, in-context learning, answering in\n",
      "zero-shot settings, etc. These abilities are known to be ac-\n",
      "quired by them due to their gigantic scale even when the pre-\n",
      "trained LLMs are not trained specifically to possess these at-\n",
      "Summary: \n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\" Please summarize the below docs: \n",
      "docs:trained LLMs are not trained specifically to possess these at-\n",
      "tributes [22, 23, 24]. Such abilities have led LLMs to be widelyadopted in diverse settings including, multi-modal, robotics,\n",
      "tool manipulation, question answering, autonomous agents, etc.\n",
      "Various improvements have also been suggested in these areas\n",
      "either by task-specific training [25, 26, 27, 28, 29, 30, 31] or\n",
      "better prompting [32].\n",
      "The LLMs abilities to solve diverse tasks with human-level\n",
      "Summary: \n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\" Please summarize the below docs: \n",
      "docs:better prompting [32].\n",
      "The LLMs abilities to solve diverse tasks with human-level\n",
      "performance come at a cost of slow training and inference,\n",
      "extensive hardware requirements, and higher running costs.\n",
      "Such requirements have limited their adoption and opened up\n",
      "Summary: \n",
      "\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m summary_chain \u001b[38;5;241m=\u001b[39m load_summarize_chain(llm,chain_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmap_reduce\u001b[39m\u001b[38;5;124m\"\u001b[39m,map_prompt\u001b[38;5;241m=\u001b[39mmap_prompt_template, combine_prompt\u001b[38;5;241m=\u001b[39mfinal_prompt_template, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 2\u001b[0m output\u001b[38;5;241m=\u001b[39m \u001b[43msummary_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal_docs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m output\n",
      "File \u001b[1;32mc:\\Users\\mrafi\\Documents\\LangchainProjects\\1-Q&AChatbots\\AIProjects\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:170\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     emit_warning()\n\u001b[1;32m--> 170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mrafi\\Documents\\LangchainProjects\\1-Q&AChatbots\\AIProjects\\lib\\site-packages\\langchain\\chains\\base.py:598\u001b[0m, in \u001b[0;36mChain.run\u001b[1;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    597\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supports only one positional argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 598\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[0;32m    599\u001b[0m         _output_key\n\u001b[0;32m    600\u001b[0m     ]\n\u001b[0;32m    602\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[0;32m    603\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(kwargs, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, tags\u001b[38;5;241m=\u001b[39mtags, metadata\u001b[38;5;241m=\u001b[39mmetadata)[\n\u001b[0;32m    604\u001b[0m         _output_key\n\u001b[0;32m    605\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\mrafi\\Documents\\LangchainProjects\\1-Q&AChatbots\\AIProjects\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:170\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     emit_warning()\n\u001b[1;32m--> 170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mrafi\\Documents\\LangchainProjects\\1-Q&AChatbots\\AIProjects\\lib\\site-packages\\langchain\\chains\\base.py:381\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    349\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[0;32m    350\u001b[0m \n\u001b[0;32m    351\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    374\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    375\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m    378\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[0;32m    379\u001b[0m }\n\u001b[1;32m--> 381\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mrafi\\Documents\\LangchainProjects\\1-Q&AChatbots\\AIProjects\\lib\\site-packages\\langchain\\chains\\base.py:164\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    163\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 164\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    165\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[1;32mc:\\Users\\mrafi\\Documents\\LangchainProjects\\1-Q&AChatbots\\AIProjects\\lib\\site-packages\\langchain\\chains\\base.py:154\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[0;32m    153\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 154\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    155\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    156\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    157\u001b[0m     )\n\u001b[0;32m    159\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    160\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\mrafi\\Documents\\LangchainProjects\\1-Q&AChatbots\\AIProjects\\lib\\site-packages\\langchain\\chains\\combine_documents\\base.py:138\u001b[0m, in \u001b[0;36mBaseCombineDocumentsChain._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m# Other keys are assumed to be needed for LLM prediction\u001b[39;00m\n\u001b[0;32m    137\u001b[0m other_keys \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_key}\n\u001b[1;32m--> 138\u001b[0m output, extra_return_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcombine_docs(\n\u001b[0;32m    139\u001b[0m     docs, callbacks\u001b[38;5;241m=\u001b[39m_run_manager\u001b[38;5;241m.\u001b[39mget_child(), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mother_keys\n\u001b[0;32m    140\u001b[0m )\n\u001b[0;32m    141\u001b[0m extra_return_dict[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key] \u001b[38;5;241m=\u001b[39m output\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m extra_return_dict\n",
      "File \u001b[1;32mc:\\Users\\mrafi\\Documents\\LangchainProjects\\1-Q&AChatbots\\AIProjects\\lib\\site-packages\\langchain\\chains\\combine_documents\\map_reduce.py:226\u001b[0m, in \u001b[0;36mMapReduceDocumentsChain.combine_docs\u001b[1;34m(self, docs, token_max, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcombine_docs\u001b[39m(\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    216\u001b[0m     docs: List[Document],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    220\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mdict\u001b[39m]:\n\u001b[0;32m    221\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Combine documents in a map reduce manner.\u001b[39;00m\n\u001b[0;32m    222\u001b[0m \n\u001b[0;32m    223\u001b[0m \u001b[38;5;124;03m    Combine by mapping first chain over all documents, then reducing the results.\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;124;03m    This reducing can be done recursively if needed (if there are many documents).\u001b[39;00m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 226\u001b[0m     map_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# FYI - this is parallelized and so it is fast.\u001b[39;49;00m\n\u001b[0;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdocument_variable_name\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpage_content\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    231\u001b[0m     question_result_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_chain\u001b[38;5;241m.\u001b[39moutput_key\n\u001b[0;32m    232\u001b[0m     result_docs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    233\u001b[0m         Document(page_content\u001b[38;5;241m=\u001b[39mr[question_result_key], metadata\u001b[38;5;241m=\u001b[39mdocs[i]\u001b[38;5;241m.\u001b[39mmetadata)\n\u001b[0;32m    234\u001b[0m         \u001b[38;5;66;03m# This uses metadata from the docs, and the textual results from `results`\u001b[39;00m\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i, r \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(map_results)\n\u001b[0;32m    236\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\mrafi\\Documents\\LangchainProjects\\1-Q&AChatbots\\AIProjects\\lib\\site-packages\\langchain\\chains\\llm.py:250\u001b[0m, in \u001b[0;36mLLMChain.apply\u001b[1;34m(self, input_list, callbacks)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    249\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 250\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    251\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_outputs(response)\n\u001b[0;32m    252\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m\"\u001b[39m: outputs})\n",
      "File \u001b[1;32mc:\\Users\\mrafi\\Documents\\LangchainProjects\\1-Q&AChatbots\\AIProjects\\lib\\site-packages\\langchain\\chains\\llm.py:247\u001b[0m, in \u001b[0;36mLLMChain.apply\u001b[1;34m(self, input_list, callbacks)\u001b[0m\n\u001b[0;32m    242\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[0;32m    243\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    244\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_list\u001b[39m\u001b[38;5;124m\"\u001b[39m: input_list},\n\u001b[0;32m    245\u001b[0m )\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 247\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    249\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32mc:\\Users\\mrafi\\Documents\\LangchainProjects\\1-Q&AChatbots\\AIProjects\\lib\\site-packages\\langchain\\chains\\llm.py:138\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[1;34m(self, input_list, run_manager)\u001b[0m\n\u001b[0;32m    136\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m run_manager\u001b[38;5;241m.\u001b[39mget_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm, BaseLanguageModel):\n\u001b[1;32m--> 138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    139\u001b[0m         prompts,\n\u001b[0;32m    140\u001b[0m         stop,\n\u001b[0;32m    141\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_kwargs,\n\u001b[0;32m    143\u001b[0m     )\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    145\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mbind(stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_kwargs)\u001b[38;5;241m.\u001b[39mbatch(\n\u001b[0;32m    146\u001b[0m         cast(List, prompts), {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks}\n\u001b[0;32m    147\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\mrafi\\Documents\\LangchainProjects\\1-Q&AChatbots\\AIProjects\\lib\\site-packages\\langchain_core\\language_models\\llms.py:701\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    693\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    694\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    695\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    698\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    699\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    700\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 701\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_strings, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mrafi\\Documents\\LangchainProjects\\1-Q&AChatbots\\AIProjects\\lib\\site-packages\\langchain_core\\language_models\\llms.py:880\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    865\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    866\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    867\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    868\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    878\u001b[0m         )\n\u001b[0;32m    879\u001b[0m     ]\n\u001b[1;32m--> 880\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_helper(\n\u001b[0;32m    881\u001b[0m         prompts, stop, run_managers, \u001b[38;5;28mbool\u001b[39m(new_arg_supported), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    882\u001b[0m     )\n\u001b[0;32m    883\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[0;32m    884\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\mrafi\\Documents\\LangchainProjects\\1-Q&AChatbots\\AIProjects\\lib\\site-packages\\langchain_core\\language_models\\llms.py:738\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    736\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[0;32m    737\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 738\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    739\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m    740\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[1;32mc:\\Users\\mrafi\\Documents\\LangchainProjects\\1-Q&AChatbots\\AIProjects\\lib\\site-packages\\langchain_core\\language_models\\llms.py:725\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    715\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[0;32m    716\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    717\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    721\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    722\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    723\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    724\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 725\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    726\u001b[0m                 prompts,\n\u001b[0;32m    727\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    728\u001b[0m                 \u001b[38;5;66;03m# TODO: support multiple run managers\u001b[39;00m\n\u001b[0;32m    729\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    730\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    731\u001b[0m             )\n\u001b[0;32m    732\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    733\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m    734\u001b[0m         )\n\u001b[0;32m    735\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    736\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mc:\\Users\\mrafi\\Documents\\LangchainProjects\\1-Q&AChatbots\\AIProjects\\lib\\site-packages\\langchain_community\\llms\\ollama.py:429\u001b[0m, in \u001b[0;36mOllama._generate\u001b[1;34m(self, prompts, stop, images, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    427\u001b[0m generations \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[1;32m--> 429\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_stream_with_aggregation(\n\u001b[0;32m    430\u001b[0m         prompt,\n\u001b[0;32m    431\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    432\u001b[0m         images\u001b[38;5;241m=\u001b[39mimages,\n\u001b[0;32m    433\u001b[0m         run_manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[0;32m    434\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m    435\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    436\u001b[0m     )\n\u001b[0;32m    437\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([final_chunk])\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[1;32mc:\\Users\\mrafi\\Documents\\LangchainProjects\\1-Q&AChatbots\\AIProjects\\lib\\site-packages\\langchain_community\\llms\\ollama.py:347\u001b[0m, in \u001b[0;36m_OllamaCommon._stream_with_aggregation\u001b[1;34m(self, prompt, stop, run_manager, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_stream_with_aggregation\u001b[39m(\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    340\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    345\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m GenerationChunk:\n\u001b[0;32m    346\u001b[0m     final_chunk: Optional[GenerationChunk] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m stream_resp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_generate_stream(prompt, stop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    348\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m stream_resp:\n\u001b[0;32m    349\u001b[0m             chunk \u001b[38;5;241m=\u001b[39m _stream_response_to_generation_chunk(stream_resp)\n",
      "File \u001b[1;32mc:\\Users\\mrafi\\Documents\\LangchainProjects\\1-Q&AChatbots\\AIProjects\\lib\\site-packages\\langchain_community\\llms\\ollama.py:192\u001b[0m, in \u001b[0;36m_OllamaCommon._create_generate_stream\u001b[1;34m(self, prompt, stop, images, **kwargs)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_generate_stream\u001b[39m(\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    186\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    190\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    191\u001b[0m     payload \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m: images}\n\u001b[1;32m--> 192\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_stream(\n\u001b[0;32m    193\u001b[0m         payload\u001b[38;5;241m=\u001b[39mpayload,\n\u001b[0;32m    194\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    195\u001b[0m         api_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/api/generate\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    196\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    197\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\mrafi\\Documents\\LangchainProjects\\1-Q&AChatbots\\AIProjects\\lib\\site-packages\\requests\\models.py:869\u001b[0m, in \u001b[0;36mResponse.iter_lines\u001b[1;34m(self, chunk_size, decode_unicode, delimiter)\u001b[0m\n\u001b[0;32m    860\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Iterates over the response data, one line at a time.  When\u001b[39;00m\n\u001b[0;32m    861\u001b[0m \u001b[38;5;124;03mstream=True is set on the request, this avoids reading the\u001b[39;00m\n\u001b[0;32m    862\u001b[0m \u001b[38;5;124;03mcontent at once into memory for large responses.\u001b[39;00m\n\u001b[0;32m    863\u001b[0m \n\u001b[0;32m    864\u001b[0m \u001b[38;5;124;03m.. note:: This method is not reentrant safe.\u001b[39;00m\n\u001b[0;32m    865\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    867\u001b[0m pending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 869\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_content(\n\u001b[0;32m    870\u001b[0m     chunk_size\u001b[38;5;241m=\u001b[39mchunk_size, decode_unicode\u001b[38;5;241m=\u001b[39mdecode_unicode\n\u001b[0;32m    871\u001b[0m ):\n\u001b[0;32m    872\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pending \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    873\u001b[0m         chunk \u001b[38;5;241m=\u001b[39m pending \u001b[38;5;241m+\u001b[39m chunk\n",
      "File \u001b[1;32mc:\\Users\\mrafi\\Documents\\LangchainProjects\\1-Q&AChatbots\\AIProjects\\lib\\site-packages\\requests\\utils.py:572\u001b[0m, in \u001b[0;36mstream_decode_response_unicode\u001b[1;34m(iterator, r)\u001b[0m\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    571\u001b[0m decoder \u001b[38;5;241m=\u001b[39m codecs\u001b[38;5;241m.\u001b[39mgetincrementaldecoder(r\u001b[38;5;241m.\u001b[39mencoding)(errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplace\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 572\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m iterator:\n\u001b[0;32m    573\u001b[0m     rv \u001b[38;5;241m=\u001b[39m decoder\u001b[38;5;241m.\u001b[39mdecode(chunk)\n\u001b[0;32m    574\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rv:\n",
      "File \u001b[1;32mc:\\Users\\mrafi\\Documents\\LangchainProjects\\1-Q&AChatbots\\AIProjects\\lib\\site-packages\\requests\\models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[1;32mc:\\Users\\mrafi\\Documents\\LangchainProjects\\1-Q&AChatbots\\AIProjects\\lib\\site-packages\\urllib3\\response.py:1057\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m   1041\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;124;03mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[0;32m   1043\u001b[0m \u001b[38;5;124;03m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1054\u001b[0m \u001b[38;5;124;03m    'content-encoding' header.\u001b[39;00m\n\u001b[0;32m   1055\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1056\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupports_chunked_reads():\n\u001b[1;32m-> 1057\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_chunked(amt, decode_content\u001b[38;5;241m=\u001b[39mdecode_content)\n\u001b[0;32m   1058\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1059\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\mrafi\\Documents\\LangchainProjects\\1-Q&AChatbots\\AIProjects\\lib\\site-packages\\urllib3\\response.py:1206\u001b[0m, in \u001b[0;36mHTTPResponse.read_chunked\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m   1203\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1205\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1206\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_chunk_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1207\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1208\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mrafi\\Documents\\LangchainProjects\\1-Q&AChatbots\\AIProjects\\lib\\site-packages\\urllib3\\response.py:1125\u001b[0m, in \u001b[0;36mHTTPResponse._update_chunk_length\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1125\u001b[0m line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[0;32m   1126\u001b[0m line \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\mrafi\\Documents\\LangchainProjects\\1-Q&AChatbots\\AIProjects\\lib\\socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "summary_chain = load_summarize_chain(llm,chain_type=\"map_reduce\",map_prompt=map_prompt_template, combine_prompt=final_prompt_template, verbose=True)\n",
    "output= summary_chain.run(final_docs)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RefineDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"A Comprehensive Overview of Large Language Models\n",
      "Humza Naveeda, Asad Ullah Khana,∗, Shi Qiub,∗, Muhammad Saqibc,d,∗, Saeed Anware,f, Muhammad Usmane,f, Naveed Akhtarg,i,\n",
      "Nick Barnesh, Ajmal Miani\n",
      "aUniversity of Engineering and Technology (UET), Lahore, Pakistan\n",
      "bThe Chinese University of Hong Kong (CUHK), HKSAR, China\n",
      "cUniversity of Technology Sydney (UTS), Sydney, Australia\n",
      "dCommonwealth Scientific and Industrial Research Organisation (CSIRO), Sydney, Australia\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: There is no summary to provide as the text appears to be an abstract or title page with author affiliations. It does not contain a concise summary of any content. If you'd like, I can assist in writing a hypothetical concise summary based on what might be expected from such an overview, but please note that it would be speculative.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "dCommonwealth Scientific and Industrial Research Organisation (CSIRO), Sydney, Australia\n",
      "eKing Fahd University of Petroleum and Minerals (KFUPM), Dhahran, Saudi Arabia\n",
      "fSDAIA-KFUPM Joint Research Center for Artificial Intelligence (JRCAI), Dhahran, Saudi Arabia\n",
      "gThe University of Melbourne (UoM), Melbourne, Australia\n",
      "hAustralian National University (ANU), Canberra, Australia\n",
      "iThe University of Western Australia (UWA), Perth, Australia\n",
      "Abstract\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: Unfortunately, there is still no summary to provide as the text appears to be an abstract or title page with author affiliations and a heading that reads \"Abstract\". It does not contain a concise summary of any content.\n",
      "\n",
      "However, if you'd like, I can assist in writing a hypothetical concise summary based on what might be expected from such an overview. Please let me know if you'd like to proceed with this option.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "iThe University of Western Australia (UWA), Perth, Australia\n",
      "Abstract\n",
      "Large Language Models (LLMs) have recently demonstrated remarkable capabilities in natural language processing tasks and\n",
      "beyond. This success of LLMs has led to a large influx of research contributions in this direction. These works encompass diverse\n",
      "topics such as architectural innovations, better training strategies, context length improvements, fine-tuning, multi-modal LLMs,\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: Since the context provided seems to be relevant and sheds some light on the content of the abstract, I'll attempt to refine the original summary.\n",
      "\n",
      "The original summary was: \"Unfortunately, there is still no summary to provide as the text appears to be an abstract or title page with author affiliations and a heading that reads 'Abstract'. It does not contain a concise summary of any content.\"\n",
      "\n",
      "However, given the context below the abstract:\n",
      "\n",
      "\"The University of Western Australia (UWA), Perth, Australia\n",
      "Abstract\n",
      "Large Language Models (LLMs) have recently demonstrated remarkable capabilities in natural language processing tasks and\n",
      "beyond. This success of LLMs has led to a large influx of research contributions in this direction. These works encompass diverse\n",
      "topics such as architectural innovations, better training strategies, context length improvements, fine-tuning, multi-modal LLMs,\n",
      "\n",
      "I would refine the summary to:\n",
      "\n",
      "\"There is still no concise summary provided for the abstract, which focuses on the recent advancements and impact of Large Language Models (LLMs) in natural language processing tasks. However, it appears to be an introduction or overview of a topic area, rather than a standalone summary.\"\n",
      "\n",
      "Please let me know if this refined summary meets your requirements!\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "robotics, datasets, benchmarking, e fficiency, and more. With the rapid development of techniques and regular breakthroughs in\n",
      "LLM research, it has become considerably challenging to perceive the bigger picture of the advances in this direction. Considering\n",
      "the rapidly emerging plethora of literature on LLMs, it is imperative that the research community is able to benefit from a concise\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: The refined summary still seems necessary:\n",
      "\n",
      "\"There is still no concise summary provided for the abstract, which focuses on the recent advancements and impact of Large Language Models (LLMs) in natural language processing tasks. However, it appears to be an introduction or overview of a topic area, rather than a standalone summary.\"\n",
      "\n",
      "However, with the new context, you've added additional information about the current challenges in perceiving the bigger picture due to the rapid development of LLM research and the need for the community to benefit from concise overviews.\n",
      "\n",
      "In this case, I'd like to refine the summary slightly further:\n",
      "\n",
      "\"There is still no concise summary provided for the abstract, which focuses on the recent advancements and impact of Large Language Models (LLMs) in natural language processing tasks. However, it appears to be an introduction or overview of a topic area, rather than a standalone summary. The text hints at the complexities and challenges arising from the rapid growth of LLM research, underscoring the need for comprehensive summaries or reviews.\"\n",
      "\n",
      "Let me know if this revised summary meets your requirements!\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "yet comprehensive overview of the recent developments in this field. This article provides an overview of the existing literature\n",
      "on a broad range of LLM-related concepts. Our self-contained comprehensive overview of LLMs discusses relevant background\n",
      "concepts along with covering the advanced topics at the frontier of research in LLMs. This review article is intended to not only\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: The revised summary meets your requirements!\n",
      "\n",
      "Here's the refined summary:\n",
      "\n",
      "\"There is still no concise summary provided for the abstract, which focuses on the recent advancements and impact of Large Language Models (LLMs) in natural language processing tasks. However, it appears to be an introduction or overview of a topic area, rather than a standalone summary. The text hints at the complexities and challenges arising from the rapid growth of LLM research, underscoring the need for comprehensive summaries or reviews.\"\n",
      "\n",
      "I didn't find any additional context that would require refinements beyond what we already discussed. If anything, the provided context further emphasizes the importance of concise overviews in this field!\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "provide a systematic survey but also a quick comprehensive reference for the researchers and practitioners to draw insights from\n",
      "extensive informative summaries of the existing works to advance the LLM research.\n",
      "Keywords:\n",
      "Large Language Models, LLMs, chatGPT, Augmented LLMs, Multimodal LLMs, LLM training, LLM Benchmarking\n",
      "1. Introduction\n",
      "Language plays a fundamental role in facilitating commu-\n",
      "nication and self-expression for humans, and their interaction\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: I'm glad we refined the original summary to meet your requirements!\n",
      "\n",
      "After reviewing the additional context, I believe a refinement would be beneficial. Here's an updated summary:\n",
      "\n",
      "\"There is still no concise summary provided for the abstract, which focuses on the recent advancements and impact of Large Language Models (LLMs) in natural language processing tasks. However, it appears to be an introduction or overview of a topic area, rather than a standalone summary. The text hints at the complexities and challenges arising from the rapid growth of LLM research, underscoring the need for comprehensive summaries or reviews. This is particularly evident in the call for extensive informative summaries of existing works to advance LLM research, highlighting the importance of synthesizing knowledge to facilitate insights for researchers and practitioners.\"\n",
      "\n",
      "I refined the original summary by incorporating a sentence that reflects the additional context's emphasis on the need for comprehensive summaries to advance LLM research. The updated summary maintains its concise nature while providing a clearer understanding of the implications of not having such summaries in this field.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "nication and self-expression for humans, and their interaction\n",
      "with machines. The need for generalized models stems from\n",
      "the growing demand for machines to handle complex language\n",
      "tasks, including translation, summarization, information re-\n",
      "trieval, conversational interactions, etc. Recently, significant\n",
      "breakthroughs have been witnessed in language models, pri-\n",
      "marily attributed to transformers [1], increased computational\n",
      "capabilities, and the availability of large-scale training data.\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: I'm glad we had a chance to revisit the summary.\n",
      "\n",
      "After reviewing the additional context, I believe a refinement is indeed beneficial. Here's an updated summary:\n",
      "\n",
      "\"Recent advancements in Large Language Models (LLMs) have revolutionized natural language processing tasks, with significant breakthroughs attributed to transformers, increased computational capabilities, and large-scale training data. However, the growing complexity of LLM research underscores the need for comprehensive summaries or reviews to synthesize knowledge and facilitate insights for researchers and practitioners.\"\n",
      "\n",
      "I refined the original summary by incorporating a sentence that reflects the additional context's emphasis on the recent advancements in LLMs and the need for comprehensive summaries to advance this field. The updated summary maintains its concise nature while providing a clearer understanding of the current state of LLM research and its implications.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "capabilities, and the availability of large-scale training data.\n",
      "These developments have brought about a revolutionary trans-\n",
      "formation by enabling the creation of LLMs that can approxi-\n",
      "mate human-level performance on various tasks [2, 3]. Large\n",
      "∗Equal contribution\n",
      "Email addresses: humza_naveed@yahoo.com (Humza Naveed),\n",
      "aukhanee@gmail.com (Asad Ullah Khan), shiqiu@cse.cuhk.edu.hk (Shi\n",
      "Qiu), muhammad.saqib@data61.csiro.au (Muhammad Saqib),\n",
      "saeed.anwar@kfupm.edu.sa (Saeed Anwar),\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: I believe a refinement is indeed beneficial. Here's an updated summary:\n",
      "\n",
      "\"Recent advancements in Large Language Models (LLMs) have revolutionized natural language processing tasks, with significant breakthroughs attributed to transformers, increased computational capabilities, and large-scale training data. These developments have brought about a revolutionary transformation by enabling the creation of LLMs that can approximate human-level performance on various tasks [2, 3]. As a result, the growing complexity of LLM research underscores the need for comprehensive summaries or reviews to synthesize knowledge and facilitate insights for researchers and practitioners.\"\n",
      "\n",
      "I refined the original summary by incorporating a sentence from the additional context that highlights the specific developments (transformers, computational capabilities, and large-scale training data) that have led to the creation of highly capable LLMs. The updated summary maintains its concise nature while providing a clearer understanding of the current state of LLM research and its implications.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "Qiu), muhammad.saqib@data61.csiro.au (Muhammad Saqib),\n",
      "saeed.anwar@kfupm.edu.sa (Saeed Anwar),\n",
      "muhammad.usman@kfupm.edu.sa (Muhammad Usman),\n",
      "naveed.akhtar1@unimelb.edu.au (Naveed Akhtar),\n",
      "nick.barnes@anu.edu.au (Nick Barnes), ajmal.mian@uwa.edu.au\n",
      "(Ajmal Mian)\n",
      "Figure 1: The trend of papers released over years containing keywords \"Large\n",
      "Language Model\", \"Large Language Model +Fine-Tuning\", and \"Large Lan-\n",
      "guage Model +Alignment\".\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: Here's a refined summary that incorporates the additional context:\n",
      "\n",
      "\"Recent advancements in Large Language Models (LLMs) have revolutionized natural language processing tasks, with significant breakthroughs attributed to transformers, increased computational capabilities, and large-scale training data. As shown by the trend of papers released over the years (Fig. 1), this growth has led to a substantial increase in research on fine-tuning and alignment of LLMs. These developments have brought about a revolutionary transformation by enabling the creation of LLMs that can approximate human-level performance on various tasks [2, 3]. The growing complexity of LLM research underscores the need for comprehensive summaries or reviews to synthesize knowledge and facilitate insights for researchers and practitioners.\"\n",
      "\n",
      "I added a sentence that references Figure 1, highlighting the trend of increasing research in fine-tuning and alignment of LLMs. This provides additional context and reinforces the importance of refining our understanding of LLM research through comprehensive summaries or reviews.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "Language Model\", \"Large Language Model +Fine-Tuning\", and \"Large Lan-\n",
      "guage Model +Alignment\".\n",
      "Preprint submitted to Elsevier April 11, 2024arXiv:2307.06435v9  [cs.CL]  9 Apr 2024\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: Here's a refined summary that incorporates the additional context:\n",
      "\n",
      "\"Recent advancements in Large Language Models (LLMs) have revolutionized natural language processing tasks, with significant breakthroughs attributed to transformers, increased computational capabilities, and large-scale training data. As shown by the trend of papers released over the years (Fig. 1), this growth has led to a substantial increase in research on fine-tuning and alignment of LLMs. These developments have brought about a revolutionary transformation by enabling the creation of LLMs that can approximate human-level performance on various tasks [2, 3]. Furthermore, the distinction between LLMs, Large Language Model +Fine-Tuning, and Large Language Model +Alignment highlights the complexity of this research area, underscoring the need for comprehensive summaries or reviews to synthesize knowledge and facilitate insights for researchers and practitioners.\"\n",
      "\n",
      "I refined the original summary by adding a sentence that references the differentiation between LLMs and related concepts (Large Language Model +Fine-Tuning and Large Language Model +Alignment), reinforcing the importance of refining our understanding of LLM research through comprehensive summaries or reviews.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "2019T5(Oct)\n",
      "GPT-3(May)\n",
      " WebGPT (Dec)\n",
      "OPT -IML\n",
      "TK-Instruct (May)\n",
      "mT0 (Dec)\n",
      " Wizard -LM\n",
      "Vicuna\n",
      "Alpaca (Mar)\n",
      "HuaTuo (Apr)\n",
      "Koala (May)\n",
      "Wizard -Coder (Jun)\n",
      "Goat\n",
      "PanGu -α(Apr)\n",
      "CPM -2(Jun)\n",
      "GPT-NeoX -20B (Apr)\n",
      "CodeGen (Mar)\n",
      "Galactica (Nov)\n",
      "GLM (Oct)\n",
      "OPT\n",
      "UL2 (May)\n",
      "LLaMA (Feb)\n",
      "LLaMA 2(Jul)\n",
      "MPT (Jun)\n",
      "CodeT5+\n",
      "Code Llama (Aug)\n",
      "StarCoder\n",
      "Xuan Yuan 2.0 (May)\n",
      "2020 2021 2022 2023 2024mT5 (Oct)\n",
      "HyperCLOVA (Sep)\n",
      "ERNIE 3.0\n",
      "Codex (Jul)\n",
      "Jurassic -1(Aug)\n",
      "Yuan 1.0 (Oct)\n",
      "Gopher (Dec)\n",
      "ERNIE 3.0 Titan\n",
      "GLaM\n",
      "LaMDA\n",
      "T0(Oct)\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: Here's the refined summary:\n",
      "\n",
      "\"Recent advancements in Large Language Models (LLMs) have revolutionized natural language processing tasks, with significant breakthroughs attributed to transformers, increased computational capabilities, and large-scale training data. As shown by the trend of papers released over the years (Fig. 1), this growth has led to a substantial increase in research on fine-tuning and alignment of LLMs. These developments have brought about a revolutionary transformation by enabling the creation of LLMs that can approximate human-level performance on various tasks [2, 3]. Furthermore, the trend of LLM releases since 2019 (listed below) highlights an accelerating pace of innovation, with notable models such as GPT-3, OPT, and LLaMA achieving state-of-the-art results. The distinction between LLMs, Large Language Model +Fine-Tuning, and Large Language Model +Alignment emphasizes the complexity of this research area, underscoring the need for comprehensive summaries or reviews to synthesize knowledge and facilitate insights for researchers and practitioners.\"\n",
      "\n",
      "I refined the original summary by adding a sentence that references the trend of LLM releases since 2019, highlighting the accelerating pace of innovation in this field. I also added a brief mention of notable models such as GPT-3, OPT, and LLaMA to provide more context on the recent advancements in LLMs.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "Codex (Jul)\n",
      "Jurassic -1(Aug)\n",
      "Yuan 1.0 (Oct)\n",
      "Gopher (Dec)\n",
      "ERNIE 3.0 Titan\n",
      "GLaM\n",
      "LaMDA\n",
      "T0(Oct)\n",
      "ChatGPT (Nov)\n",
      "Sparrow (Sep)\n",
      "FLAN -U-PaLM (Oct)\n",
      "Bard (Oct)\n",
      "MT-NLG (Jan)\n",
      "AlphaCode (Feb)\n",
      "Chinchilla (Mar)\n",
      "PaLM (Apr)\n",
      "U-PALM (Oct)\n",
      "BLOOM (Nov)\n",
      "AlexaTM (Aug)\n",
      " PaLM2 (May)\n",
      "GPT-4\n",
      "PanGu -Σ(Mar)\n",
      "BloombergGPT\n",
      "Claude\n",
      "Gemini (Dec)\n",
      "Figure 2: Chronological display of LLM releases: blue cards represent ‘pre-trained’ models, while orange cards correspond to ‘instruction-tuned’ models. Models\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: I've reviewed the additional context and refined the existing summary accordingly:\n",
      "\n",
      "\"Recent advancements in Large Language Models (LLMs) have revolutionized natural language processing tasks, with significant breakthroughs attributed to transformers, increased computational capabilities, and large-scale training data. As shown by the trend of papers released over the years (Fig. 1), this growth has led to a substantial increase in research on fine-tuning and alignment of LLMs. Furthermore, the accelerating pace of innovation in LLM releases since 2019, with notable models such as GPT-3, OPT, LLaMA, Codex, Chinchilla, PaLM2, BLOOM, and Gemini achieving state-of-the-art results (Fig. 2), highlights a revolutionary transformation by enabling the creation of LLMs that can approximate human-level performance on various tasks [2, 3]. The distinction between LLMs, Large Language Model +Fine-Tuning, and Large Language Model +Alignment emphasizes the complexity of this research area, underscoring the need for comprehensive summaries or reviews to synthesize knowledge and facilitate insights for researchers and practitioners.\"\n",
      "\n",
      "I added a few more notable models from the list (Codex, Chinchilla, PaLM2, BLOOM, and Gemini) to provide an updated picture of the recent advancements in LLMs. I also referenced Figure 2 from the additional context to support the claim about the accelerating pace of innovation.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "on the upper half signify open-source availability, whereas those on the bottom half are closed-source. The chart illustrates the increasing trend towards instruction-\n",
      "tuned models and open-source models, highlighting the evolving landscape and trends in natural language processing research.\n",
      "Language Models (LLMs) have emerged as cutting-edge arti-\n",
      "ficial intelligence systems that can process and generate text\n",
      "with coherent communication [4], and generalize to multiple\n",
      "tasks [5, 6].\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: Here is the refined summary:\n",
      "\n",
      "\"Recent advancements in Large Language Models (LLMs) have revolutionized natural language processing tasks, with significant breakthroughs attributed to transformers, increased computational capabilities, and large-scale training data. As shown by the trend of papers released over the years (Fig. 1), this growth has led to a substantial increase in research on fine-tuning and alignment of LLMs. Furthermore, the accelerating pace of innovation in LLM releases since 2019, with notable models such as GPT-3, OPT, LLaMA, Codex, Chinchilla, PaLM2, BLOOM, Gemini, and other open-source models like instruction-tuned models (Fig. 2), highlights a revolutionary transformation by enabling the creation of LLMs that can approximate human-level performance on various tasks [2, 3]. The distinction between LLMs, Large Language Model +Fine-Tuning, and Large Language Model +Alignment emphasizes the complexity of this research area, underscoring the need for comprehensive summaries or reviews to synthesize knowledge and facilitate insights for researchers and practitioners.\"\n",
      "\n",
      "I added \"other open-source models like instruction-tuned models\" from the new context to provide an updated picture of the recent advancements in LLMs. The rest of the original summary remained unchanged as it was already relevant and accurate.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "with coherent communication [4], and generalize to multiple\n",
      "tasks [5, 6].\n",
      "The historical progress in natural language processing (NLP)\n",
      "evolved from statistical to neural language modeling and then\n",
      "from pre-trained language models (PLMs) to LLMs. While\n",
      "conventional language modeling (LM) trains task-specific mod-\n",
      "els in supervised settings, PLMs are trained in a self-supervised\n",
      "setting on a large corpus of text [7, 8, 9] with the aim of learning\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "summary_chain = load_summarize_chain(llm,chain_type=\"refine\",verbose=True)\n",
    "output= summary_chain.run(final_docs)\n",
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
